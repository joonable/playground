{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin-/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from datetime import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./df_dataset.pkl')\n",
    "df = df.sample(frac=1)\n",
    "df_train = df.head(int(len(df)/0.8))\n",
    "df_test = df.tail(int(len(df)/0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df.head(int(len(df)/0.8))\n",
    "df_test = df.tail(int(len(df)/0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_train.seq.values\n",
    "y_train = df_train.y.values\n",
    "x_test = df_test.seq.values\n",
    "y_test = df_test.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr':0.001,\n",
    "    'n_hidden':128,\n",
    "    'total_epoch':10,\n",
    "    'batch_size':256,\n",
    "    'embedding_size':128,\n",
    "    'n_eval':10\n",
    "}\n",
    "lr = config['lr']\n",
    "n_hidden = config['n_hidden']\n",
    "total_epoch = config['total_epoch']\n",
    "batch_size = config['batch_size']\n",
    "embedding_size = config['embedding_size']\n",
    "# self.vocabulary_list = data_helper.vocabulary_list\n",
    "# self.vocabulary_dict = data_helper.vocabulary_dict\n",
    "n_class = 2\n",
    "n_eval = config['n_eval']\n",
    "training_mode = True\n",
    "n_vocab = len(pd.read_pickle('./df_vocabulary.pkl'))\n",
    "\n",
    "global_step = tf.Variable(0, name = 'global_step', trainable = False)\n",
    "output_keep_prob = tf.placeholder(tf.float32)\n",
    "current_batch_size = tf.placeholder(dtype = tf.int32, shape = [], name = \"current_batch_size\")\n",
    "\n",
    "encoder_inputs = tf.placeholder(dtype = tf.int32, shape = [None, None], name = \"encoder_inputs\")\n",
    "decoder_inputs = tf.placeholder(dtype = tf.int64, shape = [None, None], name = \"decoder_inputs\")\n",
    "decoder_outputs = tf.placeholder(tf.int64, [None], name = \"decoder_outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-19ac8e259844>:8: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('embedding'):\n",
    "    embedding = tf.get_variable(\"embedding_layer\", [n_vocab, embedding_size], trainable = True)\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(embedding, encoder_inputs)\n",
    "    # decoder_emb_inp = tf.nn.embedding_lookup(embedding, decoder_inputs)\n",
    "    \n",
    "with tf.variable_scope('encode'):\n",
    "    encoder_length = tf.placeholder(tf.int32, [None], name = \"encoder_length\")    \n",
    "    enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob = output_keep_prob)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(cell = enc_cell, inputs = encoder_emb_inp,\n",
    "                                                      dtype = tf.float32, sequence_length = encoder_length)\n",
    "    \n",
    "with tf.variable_scope('attention'):\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        n_hidden, outputs, memory_sequence_length = encoder_length)\n",
    "\n",
    "with tf.variable_scope('decode'):\n",
    "    decoder_length = tf.placeholder(tf.int32, [None], name = \"decoder_length\")\n",
    "    target_weights = tf.placeholder(tf.float32, [None, None], name = \"target_weights\")\n",
    "\n",
    "    dec_cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob = output_keep_prob)\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        dec_cell, attention_mechanism, attention_layer_size = n_hidden)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, decoder_length)\n",
    "    projection_layer = tf.layers.Dense(n_vocab, use_bias = True)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        dec_cell, helper,\n",
    "        initial_state = dec_cell.zero_state(current_batch_size, tf.float32)\n",
    "            .clone(cell_state = enc_states),\n",
    "        output_layer = projection_layer)\n",
    "    outputs, dec_states, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''you need to build enc, dec or etc in drived classes '''\n",
    "with tf.variable_scope('output'):\n",
    "    logits = outputs.rnn_output\n",
    "    prediction = tf.argmax(logits, axis = 2)\n",
    "\n",
    "with tf.variable_scope('Cost'):\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = decoder_outputs)\n",
    "    cost = (tf.reduce_mean(crossent * target_weights))\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "with tf.variable_scope('Accuracy'):\n",
    "    # predictions = prediction * target_weights\n",
    "    correct_predictions = tf.equal(prediction, decoder_outputs)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "with tf.variable_scope('optimiser'):\n",
    "    params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(cost, params)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "    lr = tf.train.exponential_decay(lr , global_step = global_step, decay_steps = n_eval,\n",
    "                                    decay_rate = 0.999, staircase = True)\n",
    "    opimiser = tf.train.AdamOptimizer(lr)\n",
    "    train_op = opimiser.apply_gradients(\n",
    "        zip(clipped_gradients, params), global_step = global_step)\n",
    "\n",
    "graph = tf.Graph()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "train_writer = tf.summary.FileWriter('./tensorboard', sess.graph)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "set_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data_set(self, file_name):\n",
    "#     if not file_name:\n",
    "#         file_name = 'with_noise'\n",
    "#     self.df_train = pd.read_csv('./dataset/df_train_' + file_name + '.csv')\n",
    "#     self.df_test = pd.read_csv('./dataset/df_test_' + file_name + '.csv')\n",
    "# \n",
    "def set_checkpoints(self, timestamp = None):\n",
    "    # Checkpoint files will be saved in this directory during training\n",
    "    if not timestamp:\n",
    "        timestamp = str(int(time.time()))\n",
    "\n",
    "    self.timestamp = timestamp\n",
    "    self.checkpoint_dir = './checkpoints_' + self.timestamp + '/'\n",
    "    # if os.path.exists(self.checkpoint_dir):\n",
    "    #     shutil.rmtree(self.checkpoint_dir)\n",
    "    # os.makedirs(self.checkpoint_dir)\n",
    "    self.checkpoint_prefix = os.path.join(self.checkpoint_dir, 'model')\n",
    "    \n",
    "    \n",
    "def set_feed_dict(self, batch, is_train = False):\n",
    "        enc_input_batch, dec_input_batch, dec_output_batch, target_weights_batch, \\\n",
    "        enc_len_batch, dec_len_batch, current_batch_size_batch \\\n",
    "            = data_helper.make_batch(pd.DataFrame(batch, columns = ['x', 'y']))\n",
    "        if is_train:\n",
    "            output_keep_prob = 0.75\n",
    "        else:\n",
    "            output_keep_prob = 1\n",
    "\n",
    "        feed_dict = {\n",
    "            self.encoder_inputs:enc_input_batch,\n",
    "            self.decoder_inputs:dec_input_batch,\n",
    "            self.decoder_outputs:dec_output_batch,\n",
    "            self.target_weights:target_weights_batch,\n",
    "            self.encoder_length:enc_len_batch,\n",
    "            self.decoder_length:dec_len_batch,\n",
    "            self.output_keep_prob:output_keep_prob,\n",
    "            self.current_batch_size:current_batch_size_batch\n",
    "        }\n",
    "        return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    x_train = self.df_train.x.tolist()\n",
    "    y_train = self.df_train.y.tolist()\n",
    "    train_batches = data_helper.batch_iter(\n",
    "        data = list(zip(x_train, y_train)), batch_size = self.batch_size, num_epochs = self.total_epoch)\n",
    "    train_loss, train_best_loss, val_best_loss, self.best_at_step = 0, 100, 100, 0\n",
    "\n",
    "    log_msg_list = []\n",
    "\n",
    "    for train_batch in train_batches:\n",
    "        current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "        feed_dict = self.set_feed_dict(train_batch, True)\n",
    "        self.merged_summaries = tf.summary.merge_all()\n",
    "        _, loss, accuracy, summary = self.sess.run(\n",
    "            [self.train_op, self.cost, self.accuracy, self.merged_summaries], feed_dict = feed_dict)\n",
    "        self.train_writer.add_summary(summary = summary, global_step = current_step)\n",
    "\n",
    "        log_msg = 'current_step = ', '{}'.format(current_step), \\\n",
    "                  ', cost = ', '{:.6f}'.format(loss), \\\n",
    "                  ', accuracy = ', '{:.6f}'.format(accuracy)+ '\\n'\n",
    "        log_msg_list += log_msg\n",
    "        print(log_msg)\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        if current_step != 0 and current_step % self.n_eval == 0:\n",
    "            val_loss, val_accuracy = self.test(\n",
    "                self.df_test.sample(frac = 0.2).reset_index(drop = True), False)\n",
    "            train_loss /= (self.n_eval)\n",
    "            log_msg = 'current_step = ', '{}'.format(current_step), \\\n",
    "                      ', val_cost = ', '{:.6f}'.format(val_loss), \\\n",
    "                      ', val_accuracy = ', '{:.6f}'.format(val_accuracy), \\\n",
    "                      ', train_cost = ', '{:.6f}'.format(train_loss)+ '\\n'\n",
    "            log_msg_list += log_msg\n",
    "            print(log_msg)\n",
    "\n",
    "            if train_loss < train_best_loss and val_loss < val_best_loss:\n",
    "                train_best_loss, val_best_loss, self.best_at_step = train_loss, val_loss, current_step\n",
    "\n",
    "                self.save_current_session(current_step)\n",
    "\n",
    "                print('Best cost {:.6f} and {:.6f} at step {}'.format(\n",
    "                    train_best_loss, val_best_loss, self.best_at_step))\n",
    "\n",
    "            with open('./logs/log_' + self.timestamp + '.txt', 'a') as f:\n",
    "                f.writelines(log_msg_list)\n",
    "\n",
    "            log_msg_list = []\n",
    "            train_loss = 0\n",
    "            # conf = tf.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@abc.abstractmethod\n",
    "def save_current_session(self, current_step):\n",
    "    pass\n",
    "\n",
    "@abc.abstractmethod\n",
    "def restore_best_session(self, best_at_step = None):\n",
    "    pass\n",
    "\n",
    "def test(self, df, file_name, is_eval = True):\n",
    "    feed_dict = self.set_feed_dict(df, True)\n",
    "\n",
    "    results, loss, accuracy = self.sess.run([self.prediction, self.cost, self.accuracy], feed_dict = feed_dict)\n",
    "\n",
    "    if is_eval:\n",
    "        return loss, accuracy\n",
    "    else:\n",
    "        print('cost = ', '{:.6f}'.format(loss), ', accuracy = ', '{:.6f}'.format(accuracy))\n",
    "\n",
    "        decoded_number = []\n",
    "        for result in results:\n",
    "            decoded_number.append([self.vocabulary_list[i] for i in result])\n",
    "\n",
    "        decoded_jamo = []\n",
    "        for result in decoded_number:\n",
    "            try:\n",
    "                end = result.index('E')\n",
    "                decoded_jamo.append(''.join(result[:end]))\n",
    "            except:\n",
    "                decoded_jamo.append(''.join(result))\n",
    "\n",
    "        df['predict'] = [jamo.join_jamos(x) for x in decoded_jamo]\n",
    "        df.to_csv('./model_result/' + file_name + '_result_' + self.timestamp + '.csv', index = False)\n",
    "tf.keras.layers.Dense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
